{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Excellent\n",
    "https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/\n",
    "https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "https://stats.stackexchange.com/questions/204141/difference-between-selecting-features-based-on-f-regression-and-based-on-r2\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Links:\n",
    "\n",
    "https://github.com/Yorko/mlcourse_open/blob/master/jupyter_english/topic06_features/topic6_feature_engineering_feature_selection.ipynb\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "    \n",
    "    - Reducing overfitting (Variance)\n",
    "    - Interpretability\n",
    "    - Inference speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Feature Selection\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "    - Filter\n",
    "        - Statistical\n",
    "        - Model based\n",
    "    - Wrapper\n",
    "        - Forward Selection\n",
    "        - Backward Elimination\n",
    "    - Embedded\n",
    "        - Lasso\n",
    "        - Random Forest\n",
    "        - Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Feature Selection\n",
    "    - Filters out input features that do not carry information about output features\n",
    "    - Statistical approches\n",
    "    - ML algorithm approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "x_data_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "print(VarianceThreshold(.2).fit_transform(x_data_generated).shape)\n",
    "print(VarianceThreshold(.7).fit_transform(x_data_generated).shape)\n",
    "print(VarianceThreshold(.9).fit_transform(x_data_generated).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select based on \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selection=SelectFromModel(LogisticRegression(), threshold=1.25) \n",
    "# Change thresholds, threshold represents distance from the mean\n",
    "\n",
    "fitted_selection=selection.fit(x_data_generated, y_data_generated)\n",
    "\n",
    "fitted_selection.transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.551842364751\n",
      "-0.699983459749\n",
      "-0.311079205877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Pipelines\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "\n",
    "# Creates LR modes\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#Creates RF model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Creates pipe. Selection is based on RF and modeling (predictions) on LR. There is another way for creating pipeline\n",
    "# Will be shown in next example\n",
    "\n",
    "pipe = make_pipeline(SelectFromModel(estimator=RandomForestClassifier()), LogisticRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.551842364751\n",
      "-0.724338133196\n",
      "-0.874929124628\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluation of RF, LR and Pipe (combination of RF and LR)\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(lr, x_data_generated, y_data_generated, scoring='neg_log_loss').mean())\n",
    "print(cross_val_score(rf, x_data_generated, y_data_generated, scoring='neg_log_loss').mean())\n",
    "print(cross_val_score(pipe, x_data_generated, y_data_generated, scoring='neg_log_loss').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.618607502552\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting and Logistic regression may also be used for feature selection\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe1 = Pipeline([('feature_selection', SelectFromModel(estimator=LogisticRegression())),\n",
    "                   ('classification', RandomForestClassifier())])\n",
    "#pipe1.fit(x_data_generated, y_data_generated)\n",
    "\n",
    "print(cross_val_score(pipe1, x_data_generated, y_data_generated, scoring='neg_log_loss').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select K best gives control on number of features\n",
    "\n",
    "# Selects K best features based on provided function (statistical or model)\n",
    "\n",
    "# Classification - f_classif (default), mutual_info_classif, chi2\n",
    "# Regression - f_regression, mutual_info_regression, select_percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelectKBest(k=2).fit(x_data_generated, y_data_generated).transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "        True, False], dtype=bool)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelectKBest(k=2).fit(x_data_generated, y_data_generated).get_support() #.transform(x_data_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best with an arbitrary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def coefs(X,y):\n",
    "    LR=LogisticRegression().fit(X,y)\n",
    "    return(np.abs(LR.coef_)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SelectKBest(score_func=coefs, k=4)\n",
    "fit=test.fit(x_data_generated, y_data_generated)\n",
    "fit.transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (L1) Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Many algorithms enable feature selection during their optimization process: LR, SVM, RF\n",
    "# Usually these algorithms are not used in combination with other Feature Selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized_cost = cost + regularization_penalty\n",
    "# larger alpha => less features\n",
    "lasso=Lasso(alpha=0.1).fit(x_data_generated, y_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning the best alpha in lasso \n",
    "lasso_cv=LassoCV(alphas=[0.1,0.2,0.3]).fit(x_data_generated, y_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.04395427,\n",
       "       -0.        ,  0.        ,  0.        ,  0.17662538,  0.        ])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04395427,  0.17662538])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.coef_[np.abs(lasso_cv.coef_)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14, 18]),)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(lasso_cv.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14, 18]),)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.abs(lasso_cv.coef_)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wrapper Feature Selection (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computationally most intensive procedure - for each combination of features model is built and evaluated\n",
    "# More accurate than filter, but not from Embedded\n",
    "# Heuristics - Backward Elimination (Recursive Feature Elimination), Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector=RFE(estimator=LogisticRegression(), n_features_to_select=5, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "  n_features_to_select=5, step=1, verbose=0)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(x_data_generated, y_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False,  True,  True, False, False, False,\n",
       "        True, False], dtype=bool)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12,  7,  4, 13,  9,  1,  3, 10,  1, 11, 16,  5,  1,  1, 14,  2,\n",
       "        6,  1, 15])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "   estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "   n_jobs=1, scoring='accuracy', step=1, verbose=0)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RFE with Cross Validation\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "selectorCV=RFECV(estimator=LogisticRegression(), step=1, cv=StratifiedKFold(5),\\\n",
    "              scoring='accuracy')\n",
    "\n",
    "\n",
    "selectorCV.fit(x_data_generated, y_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 15, 10,  7, 16, 12,  3,  6, 13,  4, 14, 19,  8,  2,  1, 17,  5,\n",
       "        9,  1, 18])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorCV.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "## ML Extend Library\n",
    "\n",
    "# !pip install mlextend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "selector = SequentialFeatureSelector(LogisticRegression(), scoring='neg_log_loss', \n",
    "                                     verbose=2, k_features=3, forward=True, n_jobs=-1)\n",
    "\n",
    "selector.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69117985,  1.06785652,  2.05245624,  0.11008769, -0.54483436,\n",
       "       -1.04298774, -0.00544058, -0.12356116,  0.36532097, -0.37298204,\n",
       "       -0.13874104, -0.18786809, -0.03069759, -0.08491215,  0.1852439 ,\n",
       "       -0.17491275,  0.12791625,  0.04778611, -0.03101822, -0.2430916 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69117985,  1.06785652,  2.05245624,  0.11008769,  0.54483436,\n",
       "        1.04298774,  0.00544058,  0.12356116,  0.36532097,  0.37298204,\n",
       "        0.13874104,  0.18786809,  0.03069759,  0.08491215,  0.1852439 ,\n",
       "        0.17491275,  0.12791625,  0.04778611,  0.03101822,  0.2430916 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs(x_data_generated, y_data_generated)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
